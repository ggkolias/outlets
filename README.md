# Data Engineering Project

## Purpose

This project implements a data pipeline that processes raw business data and transforms it into a daily reporting table for business performance analysis. The pipeline:

- **Ingests** raw CSV data containing orders, listings, outlets, organizations, platforms, ratings, and rankings
- **Orchestrates** automated data collection using Apache Airflow (specifically fetching hourly weather data from external APIs)
- **Transforms** raw data using dbt (data build tool) to clean, deduplicate, and aggregate information
- **Delivers** a comprehensive daily reporting table that enables business users to:
  - Assess daily performance metrics
  - Understand relationships between data points (e.g., ratings impact on sales)
  - Analyze business trends over time

The reporting table consolidates information on listings, outlets, organizations, platforms, daily orders (both transaction-level and aggregated), ratings (cumulative and delta), and rankings, providing a unified view for business intelligence and decision-making.

## Project Structure

```
.
├── airflow/                    # Apache Airflow configuration and DAGs
│   └── dags/                   # Airflow DAG definitions
│       └── fetch_weather_data.py
│
├── dbt/                        # dbt project for data transformations
│   ├── models/
│   │   ├── staging/            # Staging models (cleaned raw data)
│   │   │   ├── stg_*.sql       # Staging SQL models
│   │   │   └── schema.yml      # Staging model tests and documentation
│   │   └── marts/              # Mart models (business logic)
│   │       ├── daily_reporting_table.sql
│   │       └── schema.yml      # Mart model tests and documentation
│   ├── macros/                 # Custom dbt macros
│   ├── tests/                  # Custom SQL tests
│   ├── packages.yml            # dbt package dependencies
│   └── dbt_project.yml         # dbt project configuration
│
├── infrastructure/             # Infrastructure setup and management scripts
│   ├── setup_infra.sh          # Initial infrastructure setup
│   ├── load_and_transform.sh   # Load data and run dbt transformations
│   ├── start_airflow.sh        # Start Airflow services
│   └── profiles.yml.template   # dbt profiles template
│
├── scripts/                    # Python utility scripts
│   ├── fetch_weather.py        # Weather data fetching script
│   └── load_csv_data.py        # CSV data loading script
│
├── csv_data/                   # Raw CSV data files
│   ├── orders.csv
│   ├── orders_daily.csv
│   ├── listing.csv
│   ├── outlet.csv
│   ├── org.csv
│   ├── platform.csv
│   ├── rank.csv
│   └── ratings_agg.csv
│
├── data/                       # Generated data files
│   └── weather/                # Weather CSV files (generated by Airflow)
│
├── requirements.txt            # Python dependencies
└── README.md                   # This file
```

### Key Directories

- **`airflow/`**: Contains Airflow configuration and DAGs.
- **`dbt/`**: Contains all dbt models, tests, and configuration. Models are organized into `staging/` (data cleaning) and `marts/` (business logic).
- **`infrastructure/`**: Setup and execution scripts for the entire pipeline.
- **`scripts/`**: Python scripts for data fetching and loading.
- **`csv_data/`**: Place your raw CSV data files here before running the pipeline. The files should be added by the user, in the repo the directory will be empty.
- **`data/weather/`**: Weather CSV files are generated here by the Airflow DAG.

## Prerequisites

Before setting up the infrastructure, ensure you have the following installed:

- **Python 3.9+** (Python 3.9.6 or higher recommended)
- **PostgreSQL 15+** (with `psql` command available)
- **Homebrew** (for macOS users, to install PostgreSQL if needed)

## Setup Instructions

Follow these steps to bring the infrastructure to life:

### Step 1: Install PostgreSQL (if not already installed)

**macOS (using Homebrew):**
```bash
brew install postgresql@15
brew services start postgresql@15
```

**Verify PostgreSQL is running:**
```bash
psql -l
```

### Step 2: Add Raw CSV Data

Place your raw CSV data files in the `csv_data/` directory. The required files are:
- `orders.csv`
- `orders_daily.csv`
- `listing.csv`
- `outlet.csv`
- `org.csv`
- `platform.csv`
- `rank.csv`
- `ratings_agg.csv`

### Step 3: Initial Infrastructure Setup

Run the setup script to create the virtual environment, install dependencies, set up the PostgreSQL database, and configure dbt:

```bash
./infrastructure/setup_infra.sh
```

This script will:
1. Create a Python virtual environment (`venv/`)
2. Install all Python dependencies from `requirements.txt`
3. Create the PostgreSQL database (`sl_db`) and `raw` schema
4. Copy the dbt profiles template to `~/.dbt/profiles.yml`
5. Configure Airflow paths
6. Install dbt packages

**Important:** After running `setup_infra.sh`, you need to update `~/.dbt/profiles.yml`:
- Replace `POSTGRES_USERNAME` with your PostgreSQL username (typically your system username)
- You can find your username by running: `whoami` or checking the "Owner" column in `psql -l`

### Step 4: Start Airflow

Start the Airflow services (API server, scheduler, and DAG processor):

```bash
./infrastructure/start_airflow.sh
```

The script will:
- Initialize the Airflow database if needed
- Start the Airflow API server on `http://localhost:8080`
- Start the DAG processor and scheduler

**Access Airflow UI:**
1. Open your browser and navigate to `http://localhost:8080`
2. Credentials are auto-generated by Airflow's Simple Auth Manager
3. Find the credentials in: `airflow/simple_auth_manager_passwords.json.generated`
4. Look for the `username` and `password` fields in the JSON file

**Note:** Press `Ctrl+C` in the terminal to stop Airflow services.

### Step 5: Activate and Run Weather Data DAG

Once Airflow is running:
1. Open the Airflow UI at `http://localhost:8080`
2. Log in using the credentials from `simple_auth_manager_passwords.json.generated`
3. Find the `fetch_weather_data` DAG
4. Toggle it ON (if it's paused)
5. Trigger the DAG manually or wait for it to run on schedule (hourly)

This will fetch weather data for all outlets and save CSV files to `data/weather/`.

### Step 6: Load Data and Run Transformations

After the weather data has been fetched, load all CSV data and run dbt transformations:

```bash
./infrastructure/load_and_transform.sh
```

This script will:
1. Load all CSV files (including weather data) into PostgreSQL `raw` schema
2. Run dbt models to create `staging` and `marts` schemas:
   - **`staging` schema**: Contains views that clean and deduplicate raw data
   - **`marts` schema**: Contains tables with business logic and the final reporting table
3. Run dbt tests to validate data quality

### Step 7: Verify Results

Query the final reporting table:

```bash
psql sl_db -c "SELECT * FROM marts.daily_reporting_table LIMIT 10;"
```

## Workflow Summary

The complete workflow order is:

1. **Initial Setup** → `./infrastructure/setup_infra.sh`
2. **Start Airflow** → `./infrastructure/start_airflow.sh`
3. **Run Weather DAG** → Trigger `fetch_weather_data` DAG in Airflow UI
4. **Load & Transform** → `./infrastructure/load_and_transform.sh`

After this, your reporting table (`marts.daily_reporting_table`) will be ready for analysis!

## Data Quality Checks

The project includes comprehensive data quality tests implemented using dbt. These tests are automatically executed when running `./infrastructure/load_and_transform.sh` (Step 3).

### Test Types

**1. Not Null Tests**
- Ensures required fields are never empty
- Applied to primary keys, foreign keys, and critical business fields

**2. Unique Tests**
- Validates primary key uniqueness
- Applied to `org_id`, `platform_id`, `outlet_id`, `listing_id`, `order_id`

**3. Relationship Tests**
- Validates foreign key integrity between tables
- Ensures referential integrity across staging and marts models

**4. Accepted Range Tests**
- Validates numeric values are within expected ranges:
  - **Ratings**: 0 to 5 (inclusive)
  - **Coordinates**: Latitude (-90 to 90), Longitude (-180 to 180)
  - **Weather metrics**: Wind speed (0-200), Temperature (-50 to 60°C), Humidity (0-100%)
  - **Rank values**: Non-negative

**5. Unique Combination Tests**
- Ensures no duplicate records for key combinations:
  - `(order_date, listing_id)` in daily orders
  - `(rating_date, listing_id)` in ratings
  - `(rank_date, listing_id)` in rankings
  - `(report_date, listing_id)` in reporting table

**6. Custom SQL Tests**
- **Date Range Validation**: Ensures report dates are within reasonable bounds (2020-01-01 to current date + 1 day)

### Running Tests Manually

To run data quality tests independently:

```bash
cd dbt
dbt test
```

To run tests for a specific model:

```bash
dbt test --select daily_reporting_table
```

### Test Results

All tests are defined in:
- `dbt/models/staging/schema.yml` - Tests for staging models
- `dbt/models/marts/schema.yml` - Tests for marts models
- `dbt/tests/` - Custom SQL tests

If any test fails, dbt will report the failing rows, allowing you to identify and fix data quality issues.
